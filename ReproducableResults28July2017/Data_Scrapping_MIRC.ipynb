{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Sat Jul 29 00:57:17 2017\n",
    "\n",
    "@author: pradeep sathyamurthy\n",
    "\"\"\"\n",
    "import urllib.request\n",
    "import requests\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Filter warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Type any keyword in 'query' variable\n",
    "query = ''\n",
    "data_source = \"MIRC RSNA\"\n",
    "payload = {'firstresult':'1', 'maxresults':'5000','orderby':'1','server':'0:1:2:3:4:5:6:7:8:9:10:11:12:13:14:15:16:17:18:20:21:22','document':query} # Query\n",
    "r2 = requests.post(\"http://mirc.rsna.org/query\", data=payload)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create a html file to see how it looks like\n",
    "# import webbrowser\n",
    "f = open('ALLTF.html', 'w')\n",
    "f.write(r2.text)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This function will trim the list of words passed to this function\n",
    "def Trimmed_Text(txt):\n",
    "    txt_len = len(txt)\n",
    "    final_txt = ''\n",
    "    for x in range(txt_len):\n",
    "        tmp = txt[x].getText().strip()\n",
    "        final_txt = final_txt + tmp + '\\n'\n",
    "        \n",
    "    final_txt = final_txt.strip()\n",
    "    return final_txt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def Find_RadLex_Terms(content):\n",
    "    return_val = ''\n",
    "    retrieved_info = content\n",
    "    for j in range(len(retrieved_info)):\n",
    "        # append terms (ex) abnormal chest becomes abnormal_chest\n",
    "        if len(retrieved_info[j].string.split(' ')) > 1:\n",
    "            conn_term = retrieved_info[j].string.replace(' ', '_')\n",
    "            if j == len(retrieved_info)-1:\n",
    "                return_val += conn_term\n",
    "            else:\n",
    "                return_val += conn_term+\", \"\n",
    "        else:\n",
    "            conn_term = retrieved_info[j].string\n",
    "            if j == len(retrieved_info)-1: \n",
    "                return_val += conn_term\n",
    "            else:\n",
    "                return_val += conn_term+\", \"\n",
    "    return return_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def Find_RadLex_Terms_Link(content):\n",
    "    return_val = ''\n",
    "    retrieved_info = content\n",
    "    for j in range(len(retrieved_info)):\n",
    "        if j == len(retrieved_info)-1:\n",
    "            return_val += str(retrieved_info[j]['href'])\n",
    "        else:\n",
    "            return_val += str(retrieved_info[j]['href']) + \", \"\n",
    "            \n",
    "    return return_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# A function to return texts from raw data of RSNA TF based on categories\n",
    "def RSNA_parse2(url):\n",
    "    with urllib.request.urlopen(url) as url:\n",
    "        sou = url.read()\n",
    "        soup = BeautifulSoup(sou)\n",
    "\n",
    "    # Those containers are needed for preprocessing \n",
    "    ddx_container = ['Ddx', 'Differential Diagnosis', 'Differential diagnosis', 'Differential', 'Differential Dx',\n",
    "                    'DDx', 'DDX']\n",
    "    findings_container = ['Findings (#1)', 'Findings (#2)', 'Findings']\n",
    "\n",
    "    temp2 = soup.find_all('div', class_ = \"hide\")\n",
    "\n",
    "    # Create a dataframe to store all texts\n",
    "    df = pd.DataFrame(columns = ['Data Source', 'Title', 'Authors', 'Date Created', 'History', 'Findings', \n",
    "                                 'Diagnosis', 'Discussion', 'DDX', 'References', 'RadLex_Terms', 'RadLex_Terms_Link'], index = range(0,1))\n",
    "    radlex_terms = ''\n",
    "    radles_terms_links = ''\n",
    "    for t in range(len(temp2)):\n",
    "        content = temp2[t]\n",
    "\n",
    "        if t == 0: # Retrieve author info\n",
    "            num_authors = len(content.find_all('p', class_ = 'authorname'))\n",
    "            authors = ''\n",
    "            if num_authors == 0: # If there is no author information\n",
    "                df['Authors'] = '-'\n",
    "            else:\n",
    "                for a in range(num_authors):\n",
    "                    authors += content.find_all('p', class_ = 'authorname')[a].text.strip()+\" \"\n",
    "                df['Authors'] = authors\n",
    "                # Handle date\n",
    "                dat = content.find_all(class_ = 'center')[0].text.strip().split(' ')[2]\n",
    "                if dat.isdigit(): # (1) if date format looks 20160510, convert it into 5/10/16  \n",
    "                    s = datetime(year=int(dat[0:4]), month=int(dat[4:6]), day=int(dat[6:8]))\n",
    "                    df[\"Date Created\"] = str(s.month)+\"/\"+str(s.day)+\"/\"+str(s.year)\n",
    "                elif '/' in dat: # (2) if date format looks 2005/10-22\n",
    "                    a = dat.replace('/', '-')\n",
    "                    a = \"\".join(a.split('-'))\n",
    "                    s = datetime(year=int(a[0:4]), month=int(a[4:6]), day=int(a[6:8]))\n",
    "                    df[\"Date Created\"] = str(s.month)+\"/\"+str(s.day)+\"/\"+str(s.year)\n",
    "                elif '=' in dat: # (3) if data format looks 2006-08=04\n",
    "                    a = dat.replace('=', '-')\n",
    "                    a = \"\".join(a.split('-'))\n",
    "                    s = datetime(year=int(a[0:4]), month=int(a[4:6]), day=int(a[6:8]))\n",
    "                    df[\"Date Created\"] = str(s.month)+\"/\"+str(s.day)+\"/\"+str(s.year)\n",
    "                elif '--' in dat: # (4) if data format looks 2005--1-27\n",
    "                    a = dat.replace('--', '-')\n",
    "                    a = \"\".join(a.split('-'))\n",
    "                    s = datetime(year=int(a[0:4]), month=int(a[4:6]), day=int(a[6:8]))\n",
    "                    df[\"Date Created\"] = str(s.month)+\"/\"+str(s.day)+\"/\"+str(s.year)\n",
    "                elif len(dat.split('-')[2]) > 4: # (5) if date format looks 2006-06-16000, convert it into 6/16/06\n",
    "                    a = \"\".join(dat.split('-'))\n",
    "                    s = datetime(year=int(a[0:4]), month=int(a[4:6]), day=int(a[6:8]))\n",
    "                    df[\"Date Created\"] = str(s.month)+\"/\"+str(s.day)+\"/\"+str(s.year)\n",
    "                elif len(dat.split('-')[0]) > 4: # (6) if date format looks 20005-06-16, convert it into 6/16/05. [only one]\n",
    "                    a = \"\".join(dat.split('-'))\n",
    "                    s = datetime(year=int(a[0:4]), month=int(a[5:7]), day=int(a[7:9]))\n",
    "                    df[\"Date Created\"] = str(s.month)+\"/\"+str(s.day)+\"/\"+str(2005)\n",
    "                elif len(dat.split('-')[2]) == 3: # (7) if data formae looks 2005-10-121 [only one]\n",
    "                    a = \"\".join(dat.split('-'))\n",
    "                    s = datetime(year=int(a[0:4]), month=int(a[4:6]), day=int(a[6:8]))\n",
    "                    df[\"Date Created\"] = str(s.month)+\"/\"+str(s.day)+\"/\"+str(s.year)\n",
    "                elif len(dat.split('-')[0]) == 3: # (8) if data format looks 205-06-01 [two exist]\n",
    "                    dat = dat.replace('205', '2005')\n",
    "                    a = \"\".join(dat.split('-'))\n",
    "                    s = datetime(year=int(a[0:4]), month=int(a[4:6]), day=int(a[6:8]))\n",
    "                    df[\"Date Created\"] = str(s.month)+\"/\"+str(s.day)+\"/\"+str(s.year) \n",
    "                else:\n",
    "                    df[\"Date Created\"] = dat \n",
    "        try:\n",
    "            title = content.find_all('h2')[0].getText()\n",
    "            if title in ddx_container:\n",
    "                radlex_terms += Find_RadLex_Terms(content.find_all('a'))\n",
    "                radles_terms_links += Find_RadLex_Terms_Link(content.find_all('a'))\n",
    "                df['DDX'] = Trimmed_Text(content.find_all('p'))\n",
    "            elif title in findings_container:\n",
    "                radlex_terms += Find_RadLex_Terms(content.find_all('a'))\n",
    "                radles_terms_links += Find_RadLex_Terms_Link(content.find_all('a'))\n",
    "                df['Findings'] = Trimmed_Text(content.find_all('p'))\n",
    "            elif title == 'Diagnosis':\n",
    "                radlex_terms += Find_RadLex_Terms(content.find_all('a'))\n",
    "                radles_terms_links += Find_RadLex_Terms_Link(content.find_all('a'))\n",
    "                df['Diagnosis'] = Trimmed_Text(content.find_all('p'))\n",
    "            elif title == 'Discussion':\n",
    "                radlex_terms += Find_RadLex_Terms(content.find_all('a'))\n",
    "                radles_terms_links += Find_RadLex_Terms_Link(content.find_all('a'))\n",
    "                df['Discussion'] = Trimmed_Text(content.find_all('p'))\n",
    "            elif title == 'History':\n",
    "                radlex_terms += Find_RadLex_Terms(content.find_all('a'))\n",
    "                radles_terms_links += Find_RadLex_Terms_Link(content.find_all('a'))\n",
    "                df['History'] = Trimmed_Text(content.find_all('p'))\n",
    "            elif title == 'References':\n",
    "                ref = ''\n",
    "                for r in range(len(content.find_all('li'))):\n",
    "                    ref += content.find_all('li')[r].text+\"\\n\"\n",
    "                df['References'] = ref\n",
    "        except IndexError:\n",
    "            continue\n",
    "                \n",
    "        try:\n",
    "            main_title = content.find_all('h1')[0].getText().strip()\n",
    "            df['Title'] = main_title\n",
    "        except IndexError:\n",
    "            continue\n",
    "            \n",
    "    df['RadLex_Terms'] = radlex_terms\n",
    "    df['RadLex_Terms_Link'] = radles_terms_links\n",
    "    df['Data Source'] = data_source\n",
    "    \n",
    "    df.index = [ix for ix in range(df.shape[0])]\n",
    "    \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Convert into a beautifulsoup object\n",
    "bbs = BeautifulSoup(r2.text, r\"lxml\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "grand_df = pd.DataFrame(columns = ['Data Source', 'Title', 'Authors', 'Date Created', 'History', 'Findings', \n",
    "                                   'Diagnosis', 'Discussion', 'DDX', 'References','RadLex_Terms', 'RadLex_Terms_Link'])\n",
    "for x, xml in enumerate(bbs.find_all(\"a\", href = True)):\n",
    "        dtf = RSNA_parse2(xml['href'])\n",
    "        grand_df = grand_df.append(dtf)\n",
    "\n",
    "grand_df.index = [ix for ix in range(grand_df.shape[0])]  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Remove all rows that are relevant to quiz\n",
    "grand_df = grand_df[grand_df.Title.str.contains(\"quiz\") == False]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Figure out which rows have all missing values\n",
    "x = set([x for x in range(0, grand_df.shape[0])])\n",
    "y = set(grand_df.index)\n",
    "diff = x.difference(y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF_.csv saved!!!\n"
     ]
    }
   ],
   "source": [
    "# # Create a csv file\n",
    "grand_df.to_csv('TF_'+query+'.csv')\n",
    "print('TF_'+str(query)+'.csv saved!!!')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create another csv file which rows should be ignored to download images\n",
    "\n",
    "# Aux\n",
    "diff_df = pd.DataFrame(list(diff), columns = [\"Row_IDX_to_RM\"])\n",
    "diff_df.to_csv(\"diff_df.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
