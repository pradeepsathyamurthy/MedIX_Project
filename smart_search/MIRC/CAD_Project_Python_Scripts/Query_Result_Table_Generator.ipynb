{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "ver 1.4 \n",
    "'''\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import urllib.request\n",
    "import requests\n",
    "import string\n",
    "import re\n",
    "import nltk\n",
    "from datetime import datetime, timedelta\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Filter warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Type any keyword in 'query' variable\n",
    "query = ''\n",
    "data_source = \"MIRC RSNA\"\n",
    "payload = {'firstresult':'1', 'maxresults':'5000','orderby':'1','server':'0:1:2:3:4:5:6:7:8:9:10:11:12:13:14:15:16:17:18:20:21:22','document':query} # Query\n",
    "r2 = requests.post(\"http://mirc.rsna.org/query\", data=payload)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def Trimmed_Text(txt):\n",
    "    txt_len = len(txt)\n",
    "    final_txt = ''\n",
    "    for x in range(txt_len):\n",
    "        tmp = txt[x].getText().strip()\n",
    "        final_txt = final_txt + tmp + '\\n'\n",
    "        \n",
    "    final_txt = final_txt.strip()\n",
    "    return final_txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create a html file to see how it looks like\n",
    "import webbrowser\n",
    "f = open('ALLTF.html', 'w')\n",
    "\n",
    "f.write(r2.text)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def Find_RadLex_Terms(content):\n",
    "    return_val = ''\n",
    "    retrieved_info = content\n",
    "    for j in range(len(retrieved_info)):\n",
    "        # append terms (ex) abnormal chest becomes abnormal_chest\n",
    "        if len(retrieved_info[j].string.split(' ')) > 1:\n",
    "            conn_term = retrieved_info[j].string.replace(' ', '_')\n",
    "            if j == len(retrieved_info)-1:\n",
    "                return_val += conn_term\n",
    "            else:\n",
    "                return_val += conn_term+\", \"\n",
    "        else:\n",
    "            conn_term = retrieved_info[j].string\n",
    "            if j == len(retrieved_info)-1: \n",
    "                return_val += conn_term\n",
    "            else:\n",
    "                return_val += conn_term+\", \"\n",
    "    return return_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def Find_RadLex_Terms_Link(content):\n",
    "    return_val = ''\n",
    "    retrieved_info = content\n",
    "    for j in range(len(retrieved_info)):\n",
    "        if j == len(retrieved_info)-1:\n",
    "            return_val += str(retrieved_info[j]['href'])\n",
    "        else:\n",
    "            return_val += str(retrieved_info[j]['href']) + \", \"\n",
    "            \n",
    "    return return_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# A function to return texts from raw data of RSNA TF based on categories\n",
    "def RSNA_parse2(url):\n",
    "    with urllib.request.urlopen(url) as url:\n",
    "        sou = url.read()\n",
    "        soup = BeautifulSoup(sou)\n",
    "\n",
    "    # Those containers are needed for preprocessing \n",
    "    ddx_container = ['Ddx', 'Differential Diagnosis', 'Differential diagnosis', 'Differential', 'Differential Dx',\n",
    "                    'DDx', 'DDX']\n",
    "    findings_container = ['Findings (#1)', 'Findings (#2)', 'Findings']\n",
    "\n",
    "    temp2 = soup.find_all('div', class_ = \"hide\")\n",
    "\n",
    "    # Create a dataframe to store all texts\n",
    "    df = pd.DataFrame(columns = ['Data Source', 'Title', 'Authors', 'Date Created', 'History', 'Findings', \n",
    "                                 'Diagnosis', 'Discussion', 'DDX', 'References', 'RadLex_Terms', 'RadLex_Terms_Link'], index = range(0,1))\n",
    "    radlex_terms = ''\n",
    "    radles_terms_links = ''\n",
    "    for t in range(len(temp2)):\n",
    "        content = temp2[t]\n",
    "\n",
    "        if t == 0: # Retrieve author info\n",
    "            num_authors = len(content.find_all('p', class_ = 'authorname'))\n",
    "            authors = ''\n",
    "            if num_authors == 0: # If there is no author information\n",
    "                df['Authors'] = '-'\n",
    "            else:\n",
    "                for a in range(num_authors):\n",
    "                    authors += content.find_all('p', class_ = 'authorname')[a].text.strip()+\" \"\n",
    "                df['Authors'] = authors\n",
    "                # Handle date\n",
    "                dat = content.find_all(class_ = 'center')[0].text.strip().split(' ')[2]\n",
    "                if dat.isdigit(): # (1) if date format looks 20160510, convert it into 5/10/16  \n",
    "                    s = datetime(year=int(dat[0:4]), month=int(dat[4:6]), day=int(dat[6:8]))\n",
    "                    df[\"Date Created\"] = str(s.month)+\"/\"+str(s.day)+\"/\"+str(s.year)\n",
    "                elif '/' in dat: # (2) if date format looks 2005/10-22\n",
    "                    a = dat.replace('/', '-')\n",
    "                    a = \"\".join(a.split('-'))\n",
    "                    s = datetime(year=int(a[0:4]), month=int(a[4:6]), day=int(a[6:8]))\n",
    "                    df[\"Date Created\"] = str(s.month)+\"/\"+str(s.day)+\"/\"+str(s.year)\n",
    "                elif '=' in dat: # (3) if data format looks 2006-08=04\n",
    "                    a = dat.replace('=', '-')\n",
    "                    a = \"\".join(a.split('-'))\n",
    "                    s = datetime(year=int(a[0:4]), month=int(a[4:6]), day=int(a[6:8]))\n",
    "                    df[\"Date Created\"] = str(s.month)+\"/\"+str(s.day)+\"/\"+str(s.year)\n",
    "                elif '--' in dat: # (4) if data format looks 2005--1-27\n",
    "                    a = dat.replace('--', '-')\n",
    "                    a = \"\".join(a.split('-'))\n",
    "                    s = datetime(year=int(a[0:4]), month=int(a[4:6]), day=int(a[6:8]))\n",
    "                    df[\"Date Created\"] = str(s.month)+\"/\"+str(s.day)+\"/\"+str(s.year)\n",
    "                elif len(dat.split('-')[2]) > 4: # (5) if date format looks 2006-06-16000, convert it into 6/16/06\n",
    "                    a = \"\".join(dat.split('-'))\n",
    "                    s = datetime(year=int(a[0:4]), month=int(a[4:6]), day=int(a[6:8]))\n",
    "                    df[\"Date Created\"] = str(s.month)+\"/\"+str(s.day)+\"/\"+str(s.year)\n",
    "                elif len(dat.split('-')[0]) > 4: # (6) if date format looks 20005-06-16, convert it into 6/16/05. [only one]\n",
    "                    a = \"\".join(dat.split('-'))\n",
    "                    s = datetime(year=int(a[0:4]), month=int(a[5:7]), day=int(a[7:9]))\n",
    "                    df[\"Date Created\"] = str(s.month)+\"/\"+str(s.day)+\"/\"+str(2005)\n",
    "                elif len(dat.split('-')[2]) == 3: # (7) if data formae looks 2005-10-121 [only one]\n",
    "                    a = \"\".join(dat.split('-'))\n",
    "                    s = datetime(year=int(a[0:4]), month=int(a[4:6]), day=int(a[6:8]))\n",
    "                    df[\"Date Created\"] = str(s.month)+\"/\"+str(s.day)+\"/\"+str(s.year)\n",
    "                elif len(dat.split('-')[0]) == 3: # (8) if data format looks 205-06-01 [two exist]\n",
    "                    dat = dat.replace('205', '2005')\n",
    "                    a = \"\".join(dat.split('-'))\n",
    "                    s = datetime(year=int(a[0:4]), month=int(a[4:6]), day=int(a[6:8]))\n",
    "                    df[\"Date Created\"] = str(s.month)+\"/\"+str(s.day)+\"/\"+str(s.year) \n",
    "                else:\n",
    "                    df[\"Date Created\"] = dat \n",
    "        try:\n",
    "            title = content.find_all('h2')[0].getText()\n",
    "            if title in ddx_container:\n",
    "                radlex_terms += Find_RadLex_Terms(content.find_all('a'))\n",
    "                radles_terms_links += Find_RadLex_Terms_Link(content.find_all('a'))\n",
    "                df['DDX'] = Trimmed_Text(content.find_all('p'))\n",
    "            elif title in findings_container:\n",
    "                radlex_terms += Find_RadLex_Terms(content.find_all('a'))\n",
    "                radles_terms_links += Find_RadLex_Terms_Link(content.find_all('a'))\n",
    "                df['Findings'] = Trimmed_Text(content.find_all('p'))\n",
    "            elif title == 'Diagnosis':\n",
    "                radlex_terms += Find_RadLex_Terms(content.find_all('a'))\n",
    "                radles_terms_links += Find_RadLex_Terms_Link(content.find_all('a'))\n",
    "                df['Diagnosis'] = Trimmed_Text(content.find_all('p'))\n",
    "            elif title == 'Discussion':\n",
    "                radlex_terms += Find_RadLex_Terms(content.find_all('a'))\n",
    "                radles_terms_links += Find_RadLex_Terms_Link(content.find_all('a'))\n",
    "                df['Discussion'] = Trimmed_Text(content.find_all('p'))\n",
    "            elif title == 'History':\n",
    "                radlex_terms += Find_RadLex_Terms(content.find_all('a'))\n",
    "                radles_terms_links += Find_RadLex_Terms_Link(content.find_all('a'))\n",
    "                df['History'] = Trimmed_Text(content.find_all('p'))\n",
    "            elif title == 'References':\n",
    "                ref = ''\n",
    "                for r in range(len(content.find_all('li'))):\n",
    "                    ref += content.find_all('li')[r].text+\"\\n\"\n",
    "                df['References'] = ref\n",
    "        except IndexError:\n",
    "            continue\n",
    "                \n",
    "        try:\n",
    "            main_title = content.find_all('h1')[0].getText().strip()\n",
    "            df['Title'] = main_title\n",
    "        except IndexError:\n",
    "            continue\n",
    "            \n",
    "    df['RadLex_Terms'] = radlex_terms\n",
    "    df['RadLex_Terms_Link'] = radles_terms_links\n",
    "    df['Data Source'] = data_source\n",
    "    \n",
    "    df.index = [ix for ix in range(df.shape[0])]\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF_.csv saved!!!\n"
     ]
    }
   ],
   "source": [
    "# Convert into a beautifulsoup object\n",
    "bbs = BeautifulSoup(r2.text, r\"lxml\")\n",
    "\n",
    "grand_df = pd.DataFrame(columns = ['Data Source', 'Title', 'Authors', 'Date Created', 'History', 'Findings', \n",
    "                                   'Diagnosis', 'Discussion', 'DDX', 'References','RadLex_Terms', 'RadLex_Terms_Link'])\n",
    "for x, xml in enumerate(bbs.find_all(\"a\", href = True)):\n",
    "        dtf = RSNA_parse2(xml['href'])\n",
    "        grand_df = grand_df.append(dtf)\n",
    "\n",
    "grand_df.index = [ix for ix in range(grand_df.shape[0])]        \n",
    "        \n",
    "# Remove all rows that are relevant to quiz\n",
    "grand_df = grand_df[grand_df.Title.str.contains(\"quiz\") == False]\n",
    "\n",
    "\n",
    "# Figure out which rows have all missing values\n",
    "x = set([x for x in range(0, grand_df.shape[0])])\n",
    "y = set(grand_df.index)\n",
    "diff = x.difference(y)\n",
    "\n",
    "# # Create a csv file\n",
    "grand_df.to_csv('TF_'+query+'.csv')\n",
    "print('TF_'+str(query)+'.csv saved!!!')\n",
    "\n",
    "# Create another csv file which rows should be ignored to download images\n",
    "\n",
    "# Aux\n",
    "diff_df = pd.DataFrame(list(diff), columns = [\"Row_IDX_to_RM\"])\n",
    "diff_df.to_csv(\"diff_df.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  },
  "nav_menu": {},
  "toc": {
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 6,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
