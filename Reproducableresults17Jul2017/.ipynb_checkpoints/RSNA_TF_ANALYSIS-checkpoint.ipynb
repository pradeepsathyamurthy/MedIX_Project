{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import urllib.request\n",
    "import requests\n",
    "import nltk\n",
    "import string\n",
    "import re\n",
    "import math\n",
    "import operator\n",
    "import os\n",
    "import csv\n",
    "\n",
    "from IPython.display import Image \n",
    "from itertools import compress\n",
    "from pylab import rcParams\n",
    "from collections import Counter\n",
    "from sklearn.cluster import KMeans \n",
    "from nltk.corpus import stopwords\n",
    "from sklearn import neighbors, tree\n",
    "from nltk.stem.porter import *\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn import cross_validation\n",
    "from sklearn.metrics import silhouette_score\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "from sklearn import metrics\n",
    "import pydotplus\n",
    "\n",
    "from scipy.cluster.hierarchy import fcluster\n",
    "\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage\n",
    "%matplotlib inline\n",
    "\n",
    "# Filter warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Basic Functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# A function that strips all html tags\n",
    "def striphtml(dat):\n",
    "    p = re.compile(r'<.*?>') # Regular expression for all html tags\n",
    "    return p.sub(' ', dat)\n",
    "\n",
    "# A function that modifies multiple spaces into a single space\n",
    "def removespace(dat):\n",
    "    p = re.compile(' +')\n",
    "    return p.sub(' ', dat)\n",
    "\n",
    "def get_filtered(sen):\n",
    "    sentence = sen\n",
    "    lower = sentence.lower()\n",
    "    no_punctuation = re.sub(r'[^\\w\\s]','',lower)\n",
    "    no_numbers = ''.join([w for w in no_punctuation if not w.isdigit()])\n",
    "    tokens = nltk.word_tokenize(no_numbers)\n",
    "    filtered = [w for w in tokens if not w in stopwords.words('english')]\n",
    "    return filtered\n",
    "\n",
    "def unlist(ls):\n",
    "    output = ''\n",
    "    for v in ls:\n",
    "        output = output + \" \" + v\n",
    "    return output.strip()\n",
    "\n",
    "# Euclidean Distance\n",
    "def distance(p0, p1):\n",
    "    return np.linalg.norm(p0 - p1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Collect all titles in a list\n",
    "title_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# A function to calculate the frequency of terms and return a wordcount dictionary\n",
    "def RSNA_parse2(url, threshold, radlex = False, image = False, category = False):\n",
    "    with urllib.request.urlopen(url) as url:\n",
    "        sou = url.read()\n",
    "        soup = BeautifulSoup(sou)\n",
    "\n",
    "    global kv_pairs\n",
    "    global kv_pairs2\n",
    "    global kv_pairs_cat\n",
    "    global title_list\n",
    "    \n",
    "    big_title = soup.find_all('h1')[0].text # title of TF\n",
    "    title = soup.find_all('h2') # This is a title for each category (ddx, findings...)\n",
    "    title[0] = 'Document'\n",
    "    \n",
    "    history = ''\n",
    "    discussion = ''\n",
    "    diagnosis = ''\n",
    "    findings = ''\n",
    "    ddx = ''\n",
    "    \n",
    "    # Those are the categories I want to remove\n",
    "    kill_title = [\"References\", \"Files\", \"Keywords\", \"Powerpoint Presentation\", \"Tumor Board Presetations\", \n",
    "                 \"Comments\", \"Rex shunt evaluation\", \"Ileocolic Intussusception\", \"Radiology\", \"Management\",\n",
    "                 \"Clinical Presentation\", \"Interactive Dataset I\", \"Pathology\", \"Follow-up Clinical History\",\n",
    "                 \"Instructions\", \"Document\", \"Heading\", \n",
    "                  \"AP/lateral radiographs of the right knee. AP/lateral radiographs of both knees two weeks later.\",\n",
    "                 'Images (#1)', 'Images 3 and 4', 'Image 3', 'Section Heading', 'Interactive Dataset II', \n",
    "                 'Images (#2)', 'Images 5 and 6', 'Images 4-6', 'Image 7', 'Slides', 'Images 1 and 2']\n",
    "    \n",
    "    # Those containers are needed for preprocessing \n",
    "    ddx_container = ['Ddx', 'Differential Diagnosis', 'Differential diagnosis', 'Differential', 'Differential Dx',\n",
    "                    'DDx']\n",
    "    findings_container = ['Findings (#1)', 'Findings (#2)']\n",
    "\n",
    "    # Create a title container\n",
    "    title_bag = []\n",
    "    \n",
    "    for t in title:\n",
    "        clean_title = striphtml(str(t))\n",
    "        clean_title = clean_title.strip()\n",
    "        if clean_title not in title_list: # append into the list if title does not exist\n",
    "            title_list.append(clean_title)\n",
    "\n",
    "        if clean_title in kill_title:\n",
    "            continue\n",
    "        # Preprocessing some categories. For instance, Ddx, DDX, DDx, are all synonyms and can be merged\n",
    "        elif clean_title in ddx_container:\n",
    "            clean_title = 'DDX'\n",
    "            title_bag.append(clean_title)\n",
    "        elif clean_title in findings_container:\n",
    "            clean_title = 'Findings'\n",
    "            title_bag.append(clean_title)\n",
    "        else:\n",
    "            title_bag.append(clean_title)   \n",
    "            \n",
    "    if image == True:\n",
    "        temp2 = soup.find_all('body')\n",
    "    else:\n",
    "        cnt = 0\n",
    "        temp2 = soup.find_all('div', class_ = \"hide\")\n",
    "\n",
    "        for i in range(len(title_bag)): \n",
    "            if i < (len(temp2)-1): \n",
    "                if radlex: # Find all terms that linked to RadLex  \n",
    "                    content = temp2[i+1].find_all('a')\n",
    "                    for j in range(len(content)):\n",
    "                        # append terms (ex) abnormal chest becomes abnormal_chest\n",
    "                        if len(content[j].string.split(' ')) > 1:\n",
    "                            conn_term = content[j].string.replace(' ', '_')\n",
    "                        else:\n",
    "                            conn_term = content[j].string\n",
    "                        \n",
    "                        if title_bag[i] == 'History':\n",
    "                            history = history + conn_term + ' '\n",
    "                        elif title_bag[i] == 'Findings':\n",
    "                            findings = findings + conn_term + ' '\n",
    "                        elif title_bag[i] == 'Discussion':\n",
    "                            discussion = discussion + conn_term + ' '\n",
    "                        elif title_bag[i] == 'Diagnosis':\n",
    "                            diagnosis = diagnosis + conn_term + ' '\n",
    "                        elif title_bag[i] == 'DDX':\n",
    "                            ddx = ddx + conn_term + ' '\n",
    "                            \n",
    "                        if category: # Create a dictionary based on category\n",
    "                            if title_bag[i] not in kv_pairs_cat.keys():\n",
    "                                kv_pairs_cat[title_bag[i]] = [content[j].string]\n",
    "                            else:\n",
    "                                kv_pairs_cat[title_bag[i]].append(content[j].string)\n",
    "                        else:\n",
    "                            if conn_term not in kv_pairs.keys():\n",
    "                                kv_pairs[conn_term] = 1\n",
    "                            else:\n",
    "                                kv_pairs[conn_term] += 1\n",
    "\n",
    "                else: # Find all terms\n",
    "                    content = temp2[i].text\n",
    "                    # get_filtered removed all stopwords and punctuations. \n",
    "                    clean_content = get_filtered(removespace(content))\n",
    "                    for j in range(len(clean_content)):\n",
    "                        # Skip wordcount if a term is a title\n",
    "                        if clean_content[j] in clean_title:\n",
    "                            pass\n",
    "                        else:\n",
    "                            if clean_content[j] not in kv_pairs2.keys():\n",
    "                                kv_pairs2[clean_content[j]] = 1\n",
    "                            else:\n",
    "                                kv_pairs2[clean_content[j]] += 1             \n",
    "\n",
    "        if radlex:\n",
    "            kv_pairs = dict((k,v) for k, v in dict(kv_pairs).items() if v >= threshold)\n",
    "        else:\n",
    "            kv_pairs2 = dict((k,v) for k, v in dict(kv_pairs2).items() if v >= threshold)\n",
    "            \n",
    "    return [big_title, history, findings, discussion, diagnosis, ddx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mimic POST REQUEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Type any keyword in 'query' variable\n",
    "query = ''\n",
    "payload = {'firstresult':'1', 'maxresults':'5000','orderby':'1','server':'0:1:2:3:4:5:6:7:8:9:10:11:12:13:14:15:16:17:18:20:21:22','document':query} # Query\n",
    "r2 = requests.post(\"http://mirc.rsna.org/query\", data=payload)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract xml files "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Convert into a beautifulsoup object\n",
    "bbs = BeautifulSoup(r2.text, \"lxml\")\n",
    "corpus = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Extract all xml files\n",
    "kv_pairs = {}\n",
    "kv_pairs2 = {}\n",
    "kv_pairs_cat = {}\n",
    "for xml in bbs.find_all(\"a\", href = True):\n",
    "    ret = RSNA_parse2(xml['href'], 1, radlex = True, category = True)\n",
    "#     ret_container += ret\n",
    "    corpus.append(ret)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(corpus[1])\n",
    "print(len(corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Make lists of terms\n",
    "history_words = []\n",
    "findings_words = []\n",
    "discussion_words = []\n",
    "diagnosis_words = []\n",
    "ddx_words = []\n",
    "\n",
    "title_ls = []\n",
    "\n",
    "for i in range(len(corpus)):\n",
    "    # By creating a dictionary I can keep track of the index of teaching file\n",
    "    \n",
    "    # append title\n",
    "    title_ls.append(corpus[i][0])\n",
    "    \n",
    "    if corpus[i][1] is not '':  \n",
    "        temp_dic = {}\n",
    "        temp_dic[i] = corpus[i][1]\n",
    "        history_words.append(temp_dic)\n",
    "    if corpus[i][2] is not '':\n",
    "        temp_dic = {}\n",
    "        temp_dic[i] = corpus[i][2]\n",
    "        findings_words.append(temp_dic)\n",
    "    if corpus[i][3] is not '':\n",
    "        temp_dic = {}\n",
    "        temp_dic[i] = corpus[i][3]\n",
    "        discussion_words.append(temp_dic)\n",
    "    if corpus[i][4] is not '':\n",
    "        temp_dic = {}\n",
    "        temp_dic[i] = corpus[i][4]\n",
    "        diagnosis_words.append(temp_dic)\n",
    "    if corpus[i][5] is not '':\n",
    "        temp_dic = {}\n",
    "        temp_dic[i] = corpus[i][5]\n",
    "        ddx_words.append(temp_dic)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Now create a list of words\n",
    "his_w = []\n",
    "for k in range(len(history_words)):\n",
    "    his_w.append(list(history_words[k].values())[0].strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "find_w = []\n",
    "for k in range(len(findings_words)):\n",
    "    find_w.append(list(findings_words[k].values())[0].strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dis_w = []\n",
    "for k in range(len(discussion_words)):\n",
    "    dis_w.append(list(discussion_words[k].values())[0].strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dia_w = []\n",
    "for k in range(len(diagnosis_words)):\n",
    "    dia_w.append(list(diagnosis_words[k].values())[0].strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ddx_w = []\n",
    "for k in range(len(ddx_words)):\n",
    "    ddx_w.append(list(ddx_words[k].values())[0].strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create word frequency dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ddx_freq = {}\n",
    "for tf in kv_pairs_cat['DDX']:\n",
    "    # Split word corpus \n",
    "    temp_split = tf.strip().split(' ')\n",
    "    for term in temp_split:\n",
    "        clean_term = term.lower().replace('_', ' ')\n",
    "        if clean_term in ddx_freq.keys():\n",
    "            ddx_freq[clean_term] = ddx_freq[clean_term] + 1\n",
    "        else:\n",
    "            ddx_freq[clean_term] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "his_freq = {}\n",
    "for tf in kv_pairs_cat['History']:\n",
    "    # Split word corpus \n",
    "    temp_split = tf.strip().split(' ')\n",
    "    for term in temp_split:\n",
    "        clean_term = term.lower().replace('_', ' ')\n",
    "        if clean_term in his_freq.keys():\n",
    "            his_freq[clean_term] = his_freq[clean_term] + 1\n",
    "        else:\n",
    "            his_freq[clean_term] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dis_freq = {}\n",
    "for tf in kv_pairs_cat['Discussion']:\n",
    "    # Split word corpus \n",
    "    temp_split = tf.strip().split(' ')\n",
    "    for term in temp_split:\n",
    "        clean_term = term.lower().replace('_', ' ')\n",
    "        if clean_term in dis_freq.keys():\n",
    "            dis_freq[clean_term] = dis_freq[clean_term] + 1\n",
    "        else:\n",
    "            dis_freq[clean_term] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dia_freq = {}\n",
    "for tf in kv_pairs_cat['Diagnosis']:\n",
    "    # Split word corpus \n",
    "    temp_split = tf.strip().split(' ')\n",
    "    for term in temp_split:\n",
    "        clean_term = term.lower().replace('_', ' ')\n",
    "        if clean_term in dia_freq.keys():\n",
    "            dia_freq[clean_term] = dia_freq[clean_term] + 1\n",
    "        else:\n",
    "            dia_freq[clean_term] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fin_freq = {}\n",
    "for tf in kv_pairs_cat['Findings']:\n",
    "    # Split word corpus \n",
    "    temp_split = tf.strip().split(' ')\n",
    "    for term in temp_split:\n",
    "        clean_term = term.lower().replace('_', ' ')\n",
    "        if clean_term in fin_freq.keys():\n",
    "            fin_freq[clean_term] = fin_freq[clean_term] + 1\n",
    "        else:\n",
    "            fin_freq[clean_term] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "radlex_term_freq_df_ddx = pd.Series(ddx_freq, index=ddx_freq.keys())\n",
    "radlex_term_freq_df_ddx.to_csv(\"radlex_term_Freq_ddx.csv\", )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "radlex_term_freq_df_his = pd.Series(his_freq, index=his_freq.keys())\n",
    "radlex_term_freq_df_his.to_csv(\"radlex_term_Freq_his.csv\", )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "radlex_term_freq_df_dis = pd.Series(dis_freq, index=dis_freq.keys())\n",
    "radlex_term_freq_df_dis.to_csv(\"radlex_term_Freq_dis.csv\", )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "radlex_term_freq_df_dia = pd.Series(dia_freq, index=dia_freq.keys())\n",
    "radlex_term_freq_df_dia.to_csv(\"radlex_term_Freq_dia.csv\", )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "radlex_term_freq_df_fin = pd.Series(fin_freq, index=fin_freq.keys())\n",
    "radlex_term_freq_df_fin.to_csv(\"radlex_term_Freq_fin.csv\", )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Term-frequency matrix generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Function to calculate TF-IDF\n",
    "def TFIDF_generator(w, word_list):\n",
    "    vectorizer = CountVectorizer(min_df=1)\n",
    "    X = vectorizer.fit_transform(w)\n",
    "    \n",
    "    df = pd.DataFrame(X.toarray(), columns= vectorizer.get_feature_names())\n",
    "    \n",
    "    # Get the index of History category\n",
    "    idx = [list(word_list[x].keys())[0] for x in range(len(word_list))]\n",
    "    \n",
    "    df.index = [title_ls[i] for i in idx]\n",
    "    \n",
    "    # Now I have a term-frequency matrix with TF index!!!!\n",
    "    TF = df.T\n",
    "    \n",
    "    # Calculate the number of TFs and terms\n",
    "    numTerms = TF.shape[0]\n",
    "    nTF = TF.shape[1]\n",
    "    \n",
    "    # create IDF\n",
    "    DF = np.array([(TF!=0).sum(1)]).T\n",
    "    \n",
    "    # Create a matrix with all entries to be the number of TFs\n",
    "    NMatrix=np.ones(np.shape(TF), dtype=float)*nTF\n",
    "    np.set_printoptions(precision=2,suppress=True,linewidth=120)\n",
    "\n",
    "    # Convert each entry into IDF values\n",
    "    # Note that IDF is only a function of the term, so all rows will be identical.\n",
    "    IDF = np.log2(np.divide(NMatrix, DF))\n",
    "\n",
    "    # Calculate TF-IDF\n",
    "    TFIDF = TF * IDF\n",
    "    \n",
    "    return TF, TFIDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## History "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "his_tf, his_tfidf = TFIDF_generator(his_w, history_words)\n",
    "his_tf.T.to_csv(\"his_term_freq.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Findings "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "find_tf, find_tfidf = TFIDF_generator(find_w, findings_words)\n",
    "find_tf.T.to_csv(\"find_term_freq.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discussion "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dis_tf, dis_tfidf = TFIDF_generator(dis_w, discussion_words)\n",
    "dis_tf.T.to_csv(\"dis_term_freq.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Diagnosis "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dia_tf, dia_tfidf = TFIDF_generator(dia_w, diagnosis_words)\n",
    "dia_tf.T.to_csv(\"dia_term_freq.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DDX "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ddx_tf, ddx_tfidf = TFIDF_generator(ddx_w, ddx_words)\n",
    "ddx_tf.T.to_csv(\"ddx_term_freq.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hierarchical Clustering & Dendogram interpretation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DDX "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# generate the linkage matrix\n",
    "Z = linkage(ddx_tf.T, 'ward')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(25, 10))\n",
    "plt.title('Hierarchical Clustering Dendrogram (truncated)')\n",
    "plt.xlabel('sample index or (cluster size)')\n",
    "plt.ylabel('distance')\n",
    "dendrogram(\n",
    "    Z,\n",
    "    truncate_mode='lastp',  # show only the last p merged clusters\n",
    "    p=50,  # show only the last p merged clusters\n",
    "    leaf_rotation=90.,\n",
    "    leaf_font_size=12.,\n",
    "    show_contracted=True,  # to get a distribution impression in truncated branches\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def DTree_Calculate_Accr(df, z):\n",
    "    acc_ls = []\n",
    "    \n",
    "    for t in range(2, 300, 2):\n",
    "        # Figure out the membership of each cluster\n",
    "        membership = fcluster(z, t, criterion='maxclust')\n",
    "        temp_ddx_tf = df.T\n",
    "        temp_ddx_tf['Membership'] = membership.tolist()  \n",
    "\n",
    "        # Remove all cases where the total count is 1\n",
    "        if np.sum(temp_ddx_tf.Membership.value_counts() >= 1):\n",
    "            # Extract cluster membership that does have more than one observation\n",
    "            list_a = [x for x in range(1,t+1)]\n",
    "            fil = temp_ddx_tf['Membership'].value_counts() != 1\n",
    "            # Sort by dataframe index\n",
    "            fil.sort_index(inplace=True)\n",
    "            cm_needed_ls = list(compress(list_a, fil))\n",
    "\n",
    "            # Boolean index to filter out\n",
    "            haha = [True if x in cm_needed_ls else False for x in temp_ddx_tf['Membership']]\n",
    "\n",
    "            # Our final df\n",
    "            final_dt = temp_ddx_tf[haha]\n",
    "\n",
    "            x_train, x_test, tar_train, tar_test = cross_validation.train_test_split(final_dt,\n",
    "                            final_dt['Membership'],train_size=.8, stratify=final_dt['Membership'], random_state = 33)\n",
    "        else:\n",
    "            x_train, x_test, tar_train, tar_test = cross_validation.train_test_split(temp_ddx_tf,\n",
    "                temp_ddx_tf['Membership'],train_size=.8, stratify=temp_ddx_tf['Membership'], random_state = 33)\n",
    "\n",
    "        treeclf = tree.DecisionTreeClassifier(criterion='entropy', min_samples_split=30)\n",
    "        treeclf = treeclf.fit(x_train, tar_train)\n",
    "\n",
    "        # Accuracy on Test Set\n",
    "        acc = treeclf.score(x_test, tar_test)\n",
    "        acc_ls.append(acc)\n",
    "        print('Tree ', t, ' has ',acc,' accuracy!')\n",
    "        \n",
    "#         #treecm = confusion_matrix(tar_test, treepreds_test)    \n",
    "        \n",
    "    return acc_ls, temp_ddx_tf, membership"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Scree plot for accuracy\n",
    "acc, td, memb = DTree_Calculate_Accr(his_tf, Z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Figure out the membership of each cluster\n",
    "membership = fcluster(Z, 50, criterion='maxclust')\n",
    "temp_tf = ddx_tf.T\n",
    "temp_tf['Membership'] = membership.tolist()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for c in range(1, 51):\n",
    "    temp_tf['Membership'] = membership.tolist() \n",
    "    temp_tf.loc[temp_tf['Membership'] != c, 'Membership'] = 0\n",
    "    temp_tf.loc[temp_tf['Membership'] == c, 'Membership'] = 1\n",
    "    \n",
    "    # subset for storing teaching file title for 'c' cluster\n",
    "#     subset_tf = temp_tf[temp_tf.Membership == 1]\n",
    "    \n",
    "#     for x, content in enumerate(list(subset_tf.index)):\n",
    "#         if x == 0:\n",
    "#             tf_writer.writerow(str(3)+\",\"+str(len(subset_tf.index))+\",\"+content+\"\\n\")\n",
    "#         else:\n",
    "#             print(content)\n",
    "#             tf_writer.writerow(\",,\"+content+\"\\n\")\n",
    "    \n",
    "    x_train, x_test, tar_train, tar_test = cross_validation.train_test_split(ddx_tf.T,\n",
    "        temp_tf['Membership'], train_size=.8, random_state = 33)\n",
    "\n",
    "    treeclf = tree.DecisionTreeClassifier(criterion='entropy', min_samples_split=25)\n",
    "    treeclf = treeclf.fit(temp_tf.ix[:, temp_tf.columns != 'Membership'], temp_tf['Membership'])\n",
    "    \n",
    "    # retrieve sig terms in decision tree\n",
    "    sig_feature_idx = list(set([x for x in list(treeclf.tree_.feature) if x != -2]))\n",
    "    # print terms\n",
    "    print(\"Cluster \"+str(c)+\" : \")\n",
    "    print(list(ddx_tf.index[sig_feature_idx]))\n",
    "    \n",
    "# f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Findings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "acc, td, memb = DTree_Calculate_Accr(find_tf, Z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# generate the linkage matrix\n",
    "Z = linkage(find_tf.T, 'ward')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(25, 10))\n",
    "plt.title('Hierarchical Clustering Dendrogram (truncated)')\n",
    "plt.xlabel('sample index or (cluster size)')\n",
    "plt.ylabel('distance')\n",
    "dendrogram(\n",
    "    Z,\n",
    "    truncate_mode='lastp',  # show only the last p merged clusters\n",
    "    p=40,  # show only the last p merged clusters\n",
    "    leaf_rotation=90.,\n",
    "    leaf_font_size=12.,\n",
    "    show_contracted=True,  # to get a distribution impression in truncated branches\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Diagnosis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# generate the linkage matrix\n",
    "Z = linkage(dia_tf.T, 'ward')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Scree plot for accuracy\n",
    "acc, td, memb = DTree_Calculate_Accr(dia_tf, Z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(25, 10))\n",
    "plt.title('Hierarchical Clustering Dendrogram (truncated)')\n",
    "plt.xlabel('sample index or (cluster size)')\n",
    "plt.ylabel('distance')\n",
    "dendrogram(\n",
    "    Z,\n",
    "    truncate_mode='lastp',  # show only the last p merged clusters\n",
    "    p=40,  # show only the last p merged clusters\n",
    "    leaf_rotation=90.,\n",
    "    leaf_font_size=12.,\n",
    "    show_contracted=True,  # to get a distribution impression in truncated branches\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discussion "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# generate the linkage matrix\n",
    "Z = linkage(dis_tf.T, 'ward')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(25, 10))\n",
    "plt.title('Hierarchical Clustering Dendrogram (truncated)')\n",
    "plt.xlabel('sample index or (cluster size)')\n",
    "plt.ylabel('distance')\n",
    "dendrogram(\n",
    "    Z,\n",
    "    truncate_mode='lastp',  # show only the last p merged clusters\n",
    "    p=60,  # show only the last p merged clusters\n",
    "    leaf_rotation=90.,\n",
    "    leaf_font_size=12.,\n",
    "    show_contracted=True,  # to get a distribution impression in truncated branches\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multiple Decision Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Figure out the membership of each cluster\n",
    "membership = fcluster(Z, 60, criterion='maxclust')\n",
    "temp_tf = dis_tf.T\n",
    "temp_tf['Membership'] = membership.tolist() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for c in range(1, 61):\n",
    "    temp_tf['Membership'] = membership.tolist() \n",
    "    temp_tf.loc[temp_tf['Membership'] != c, 'Membership'] = 0\n",
    "    temp_tf.loc[temp_tf['Membership'] == c, 'Membership'] = 1\n",
    "    \n",
    "    x_train, x_test, tar_train, tar_test = cross_validation.train_test_split(dis_tf.T,\n",
    "        temp_tf['Membership'], train_size=.8, random_state = 33)\n",
    "\n",
    "    treeclf = tree.DecisionTreeClassifier(criterion='entropy', min_samples_split=25)\n",
    "    treeclf = treeclf.fit(temp_tf.ix[:, temp_tf.columns != 'Membership'], temp_tf['Membership'])\n",
    "    \n",
    "    # retrieve sig terms in decision tree\n",
    "    sig_feature_idx = list(set([x for x in list(treeclf.tree_.feature) if x != -2]))\n",
    "    # print terms\n",
    "    print(\"Cluster \"+str(c)+\" : \"+dis_tf.index[sig_feature_idx])\n",
    "\n",
    "    with open(\"dis\"+str(c)+\".dot\", 'w') as f:\n",
    "        f = tree.export_graphviz(treeclf, out_file=f)\n",
    "        \n",
    "    os.unlink(\"dis\"+str(c)+\".dot\")\n",
    "    \n",
    "    dot_data = tree.export_graphviz(treeclf, out_file=None) \n",
    "    graph = pydotplus.graph_from_dot_data(dot_data) \n",
    "    graph.write_pdf(\"dis\"+str(c)+\".pdf\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Category Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DDX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def DTree_Calculate_Accr2(df, z):\n",
    "    \n",
    "    # Figure out the membership of each cluster\n",
    "    membership = fcluster(z, 50, criterion='maxclust')\n",
    "    temp_ddx_tf = df.T\n",
    "    temp_ddx_tf['Membership'] = membership.tolist()  \n",
    "\n",
    "    return temp_ddx_tf, membership"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ddf, membership = DTree_Calculate_Accr2(ddx_tf, Z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Separate into 5 big clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Big clusters\n",
    "big_clus1_df = ddf[ddf.Membership <= 3]\n",
    "big_clus2_df = ddf[ddf.Membership >= 4]\n",
    "big_clus2_df = big_clus2_df[big_clus2_df.Membership <= 7]\n",
    "big_clus3_df = ddf[ddf.Membership >= 8]\n",
    "big_clus3_df = big_clus3_df[big_clus3_df.Membership <= 25]\n",
    "big_clus4_df = ddf[ddf.Membership >= 26]\n",
    "big_clus4_df = big_clus4_df[big_clus4_df.Membership <= 29]\n",
    "big_clus5_df = ddf[ddf.Membership >= 30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "idx = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def Separate_Clusters(df):\n",
    "    global idx\n",
    "    big_clus_freq = {}\n",
    "    for row in range(df.shape[0] - 1):\n",
    "        temp_row = df.ix[row,:]\n",
    "        for a in range(df.shape[1] - 2):\n",
    "            if temp_row.index[a] not in big_clus_freq.keys():\n",
    "                big_clus_freq[temp_row.index[a]] = temp_row[a]\n",
    "            else:\n",
    "                big_clus_freq[temp_row.index[a]] += temp_row[a]   \n",
    "    big_clus = pd.Series(big_clus_freq, index=big_clus_freq.keys())\n",
    "    big_clus.to_csv(\"big_clus\"+str(idx)+\".csv\")\n",
    "    print(\"big_clus\"+str(idx)+\".csv file saved!\")\n",
    "    idx += 1    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Separate_Clusters(big_clus1_df)\n",
    "Separate_Clusters(big_clus2_df)\n",
    "Separate_Clusters(big_clus3_df)\n",
    "Separate_Clusters(big_clus4_df)\n",
    "Separate_Clusters(big_clus5_df)\n",
    "idx = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Big Cluster Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tm1 = [1 if x <= 3 else 0 for x in ddf.Membership]\n",
    "tm2 = [1 if x >= 4 and x <= 7 else 0 for x in ddf.Membership]\n",
    "tm3 = [1 if x >= 8 and x <= 25 else 0 for x in ddf.Membership]\n",
    "tm4 = [1 if x >= 26 and x <= 29 else 0 for x in ddf.Membership]\n",
    "tm5 = [1 if x >= 30 else 0 for x in ddf.Membership]\n",
    "\n",
    "cat = 'ddx'\n",
    "\n",
    "big_idx = 1\n",
    "def BigClusterAnal(tf, tm): # tf = his_tf, ddx_tf, etc\n",
    "    global big_idx\n",
    "    membership = fcluster(Z, 50, criterion='maxclust')\n",
    "    temp_tf = tf.T\n",
    "    temp_tf['Membership'] = membership.tolist() \n",
    "    \n",
    "    # Re-index Membership\n",
    "    temp_tf.Membership = tm\n",
    "    \n",
    "    # Build a decision tree\n",
    "    treeclf = tree.DecisionTreeClassifier(criterion='entropy', min_samples_split=25, min_impurity_split=0.00000002)\n",
    "    treeclf = treeclf.fit(temp_tf.ix[:, temp_tf.columns != 'Membership'], temp_tf['Membership'])\n",
    "\n",
    "    with open(\"haha_\"+str(big_idx)+\".dot\", 'w') as f:\n",
    "        f = tree.export_graphviz(treeclf, out_file=f)\n",
    "\n",
    "    os.unlink(\"haha_\"+str(big_idx)+\".dot\")\n",
    "\n",
    "    dot_data = tree.export_graphviz(treeclf, out_file=None) \n",
    "    graph = pydotplus.graph_from_dot_data(dot_data) \n",
    "    graph.write_pdf(\"haha_\"+str(big_idx)+\".pdf\")   \n",
    "    \n",
    "    term_imp = dict(zip(temp_tf.columns, treeclf.feature_importances_))\n",
    "    term_imp = sorted(term_imp.items(), key = lambda x: x[1], reverse = True) \n",
    "    \n",
    "    term_imp_df = pd.DataFrame.from_dict(term_imp, orient = \"columns\")\n",
    "    term_imp_df.columns = ['Term', 'Attr Score']\n",
    "    \n",
    "    term_imp_df['Attr Score'] = round((100 * (term_imp_df['Attr Score'] / np.max(term_imp_df['Attr Score']))))\n",
    "    \n",
    "    term_imp_df.to_csv(cat+\"_\"+str(big_idx)+\"_vs_all.csv\")\n",
    "    \n",
    "    print(cat+\"_\"+str(big_idx)+\"_vs_all.csv file saved!!\")\n",
    "    big_idx += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "BigClusterAnal(ddx_tf, tm1)\n",
    "BigClusterAnal(ddx_tf, tm2)\n",
    "BigClusterAnal(ddx_tf, tm3)\n",
    "BigClusterAnal(ddx_tf, tm4)\n",
    "BigClusterAnal(ddx_tf, tm5)\n",
    "\n",
    "big_idx = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ddf.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deeper Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 1st steel blue\n",
    "deep_big_clus1 = ddf[ddf.Membership <= 14]\n",
    "deep_big_clus1 = deep_big_clus1[deep_big_clus1.Membership >= 8]\n",
    "deep_big_clus2 = ddf[ddf.Membership == 15]\n",
    "deep_big_clus3 = ddf[ddf.Membership <= 17]\n",
    "deep_big_clus3 = deep_big_clus3[deep_big_clus3.Membership >= 16]\n",
    "deep_big_clus4 = ddf[ddf.Membership <= 25]\n",
    "deep_big_clus4 = deep_big_clus4[deep_big_clus4.Membership >= 18]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "idx = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Separate_Clusters(deep_big_clus1)\n",
    "Separate_Clusters(deep_big_clus2)\n",
    "Separate_Clusters(deep_big_clus3)\n",
    "Separate_Clusters(deep_big_clus4)\n",
    "idx = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 2nd yellow\n",
    "deep_big_clus1 = ddf[ddf.Membership <= 35]\n",
    "deep_big_clus1 = deep_big_clus1[deep_big_clus1.Membership >= 30]\n",
    "deep_big_clus2 = ddf[ddf.Membership <= 43]\n",
    "deep_big_clus2 = deep_big_clus2[deep_big_clus2.Membership >= 36]\n",
    "deep_big_clus3 = ddf[ddf.Membership <= 50]\n",
    "deep_big_clus3 = deep_big_clus3[deep_big_clus3.Membership >= 44]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "idx = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Separate_Clusters(deep_big_clus1)\n",
    "Separate_Clusters(deep_big_clus2)\n",
    "Separate_Clusters(deep_big_clus3)\n",
    "\n",
    "idx = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "membership = fcluster(Z, 50, criterion='maxclust')\n",
    "temp_tf = his_tf.T\n",
    "temp_tf['Membership'] = membership.tolist() \n",
    "\n",
    "# Remove first two irrelevant columns\n",
    "temp_tf = temp_tf.ix[:,2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ddf = temp_tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "three_most_freq_terms = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Find Clusteroids\n",
    "# Iterate through all clusters\n",
    "\n",
    "clusteroids_list = []\n",
    "size_cluster = len(np.unique(ddf['Membership']))\n",
    "\n",
    "for c in range(1,51):\n",
    "    subset_cluster = ddf[ddf['Membership'] == c]\n",
    "    \n",
    "    spec_tf = [True if x in subset_cluster.index else False for x in ddx_tf.columns]\n",
    "    term_lists = [ddx_w[i] for i, x in enumerate(spec_tf) if x]\n",
    "    wc = {}\n",
    "    for term_list in term_lists:\n",
    "        wwc = term_list.split(' ')\n",
    "        for term in wwc:\n",
    "            if term == 'trauma' or term == 'tumor' or term == 'surgery' or term == 'inguinal_hernia':\n",
    "                continue\n",
    "            else:\n",
    "                if term.lower() not in wc.keys():\n",
    "                    wc[term.lower()] = 1\n",
    "                else:\n",
    "                    wc[term.lower()] = wc[term.lower()] + 1\n",
    "\n",
    "    count_asdict = sorted(wc.items(), key = lambda x: x[1], reverse = True) \n",
    "    #print the 3 most freq terms in DDX category\n",
    "#     print(str(c))\n",
    "\n",
    "    clean_count_asdict = count_asdict\n",
    "    three_most_freq_terms.append(count_asdict[:8])\n",
    "    \n",
    "#     del wc, count_asdict, term_lists # For memory efficiency\n",
    "\n",
    "# pd.DataFrame(dict(count_asdict[:5]).keys()).to_csv(\"tmft.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Calculate within-ratio for each cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for c in range(1, 51):\n",
    "    subset_cluster = ddf[ddf['Membership'] == c]\n",
    "    km = KMeans(n_clusters=1, max_iter=1, random_state=33)\n",
    "    q = km.fit(subset_cluster.ix[:,subset_cluster.columns != \"Membership\"])\n",
    "    print(\"Cluster\"+str(c))\n",
    "    print(q.inertia_ / (subset_cluster.shape[0] - 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Separate into 3 big clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Big clusters\n",
    "big_clus1_df = ddf[ddf.Membership <= 3]\n",
    "big_clus2_df = ddf[ddf.Membership >= 4]\n",
    "big_clus2_df = big_clus2_df[big_clus2_df.Membership <= 10]\n",
    "big_clus3_df = ddf[ddf.Membership >= 11]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "idx = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def Separate_Clusters(df):\n",
    "    global idx\n",
    "    big_clus_freq = {}\n",
    "    for row in range(df.shape[0] - 1):\n",
    "        temp_row = df.ix[row,:]\n",
    "        for a in range(df.shape[1] - 2):\n",
    "            if temp_row.index[a] not in big_clus_freq.keys():\n",
    "                big_clus_freq[temp_row.index[a]] = temp_row[a]\n",
    "            else:\n",
    "                big_clus_freq[temp_row.index[a]] += temp_row[a]   \n",
    "    big_clus = pd.Series(big_clus_freq, index=big_clus_freq.keys())\n",
    "    big_clus.to_csv(\"big_clus\"+str(idx)+\".csv\")\n",
    "    print(\"big_clus\"+str(idx)+\".csv file saved!\")\n",
    "    idx += 1    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Separate_Clusters(big_clus1_df)\n",
    "Separate_Clusters(big_clus2_df)\n",
    "Separate_Clusters(big_clus3_df)\n",
    "\n",
    "idx = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Big Cluster Analysis "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tm1 = [1 if x <= 3 else 0 for x in ddf.Membership]\n",
    "tm2 = [1 if x >= 4 and x <= 10 else 0 for x in ddf.Membership]\n",
    "tm3 = [1 if x >= 11 else 0 for x in ddf.Membership]\n",
    "\n",
    "cat = 'his'\n",
    "\n",
    "big_idx = 1\n",
    "def BigClusterAnal(tf, tm): # tf = his_tf, ddx_tf, etc\n",
    "    global big_idx\n",
    "    membership = fcluster(Z, 50, criterion='maxclust')\n",
    "    temp_tf = tf.T\n",
    "    temp_tf['Membership'] = membership.tolist() \n",
    "    \n",
    "    # Re-index Membership\n",
    "    temp_tf.Membership = tm\n",
    "    \n",
    "    # Build a decision tree\n",
    "    treeclf = tree.DecisionTreeClassifier(criterion='entropy', min_samples_split=25, min_impurity_split=0.00000002)\n",
    "    treeclf = treeclf.fit(temp_tf.ix[:, temp_tf.columns != 'Membership'], temp_tf['Membership'])\n",
    "\n",
    "    with open(\"haha_\"+str(big_idx)+\".dot\", 'w') as f:\n",
    "        f = tree.export_graphviz(treeclf, out_file=f)\n",
    "\n",
    "    os.unlink(\"haha_\"+str(big_idx)+\".dot\")\n",
    "\n",
    "    dot_data = tree.export_graphviz(treeclf, out_file=None) \n",
    "    graph = pydotplus.graph_from_dot_data(dot_data) \n",
    "    graph.write_pdf(\"haha_\"+str(big_idx)+\".pdf\")   \n",
    "    \n",
    "    term_imp = dict(zip(temp_tf.columns, treeclf.feature_importances_))\n",
    "    term_imp = sorted(term_imp.items(), key = lambda x: x[1], reverse = True) \n",
    "    \n",
    "    term_imp_df = pd.DataFrame.from_dict(term_imp, orient = \"columns\")\n",
    "    term_imp_df.columns = ['Term', 'Attr Score']\n",
    "    \n",
    "    term_imp_df['Attr Score'] = round((100 * (term_imp_df['Attr Score'] / np.max(term_imp_df['Attr Score']))))\n",
    "    \n",
    "    term_imp_df.to_csv(cat+\"_\"+str(big_idx)+\"_vs_all.csv\")\n",
    "    \n",
    "    print(cat+\"_\"+str(big_idx)+\"_vs_all.csv file saved!!\")\n",
    "    big_idx += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "BigClusterAnal(his_tf, tm1)\n",
    "BigClusterAnal(his_tf, tm2)\n",
    "BigClusterAnal(his_tf, tm3)\n",
    "\n",
    "big_idx = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Findings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "membership = fcluster(Z, 40, criterion='maxclust')\n",
    "temp_tf = find_tf.T\n",
    "temp_tf['Membership'] = membership.tolist() \n",
    "\n",
    "# Remove first two irrelevant columns\n",
    "temp_tf = temp_tf.ix[:,2:]\n",
    "\n",
    "ddf = temp_tf\n",
    "three_most_freq_terms = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Find Clusteroids\n",
    "# Iterate through all clusters\n",
    "\n",
    "clusteroids_list = []\n",
    "size_cluster = len(np.unique(ddf['Membership']))\n",
    "\n",
    "for c in range(1,40):\n",
    "    subset_cluster = ddf[ddf['Membership'] == c]\n",
    "    \n",
    "    spec_tf = [True if x in subset_cluster.index else False for x in find_tf.columns]\n",
    "    term_lists = [find_w[i] for i, x in enumerate(spec_tf) if x]\n",
    "    wc = {}\n",
    "    for term_list in term_lists:\n",
    "        wwc = term_list.split(' ')\n",
    "        for term in wwc:\n",
    "            if term == 'trauma' or term == 'tumor' or term == 'surgery' or term == 'inguinal_hernia':\n",
    "                continue\n",
    "            else:\n",
    "                if term.lower() not in wc.keys():\n",
    "                    wc[term.lower()] = 1\n",
    "                else:\n",
    "                    wc[term.lower()] = wc[term.lower()] + 1\n",
    "\n",
    "    count_asdict = sorted(wc.items(), key = lambda x: x[1], reverse = True) \n",
    "    #print the 3 most freq terms in DDX category\n",
    "#     print(str(c))\n",
    "\n",
    "    clean_count_asdict = count_asdict\n",
    "    three_most_freq_terms.append(count_asdict[:8])\n",
    "    \n",
    "#     del wc, count_asdict, term_lists # For memory efficiency\n",
    "\n",
    "# pd.DataFrame(dict(count_asdict[:5]).keys()).to_csv(\"tmft.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for c in range(1, 41):\n",
    "    subset_cluster = ddf[ddf['Membership'] == c]\n",
    "    km = KMeans(n_clusters=1, max_iter=1, random_state=33)\n",
    "    q = km.fit(subset_cluster.ix[:,subset_cluster.columns != \"Membership\"])\n",
    "    print(\"Cluster\"+str(c))\n",
    "    print(q.inertia_ / (subset_cluster.shape[0] - 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Separate into 4 big clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "big_clus1_df = ddf[ddf.Membership <= 9]\n",
    "big_clus2_df = ddf[ddf.Membership >= 10]\n",
    "big_clus2_df = big_clus2_df[big_clus2_df.Membership <= 11]\n",
    "big_clus3_df = ddf[ddf.Membership >= 12]\n",
    "big_clus3_df = big_clus3_df[big_clus3_df.Membership <= 15]\n",
    "big_clus4_df = ddf[ddf.Membership >= 16]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "idx = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def Separate_Clusters(df):\n",
    "    global idx\n",
    "    big_clus_freq = {}\n",
    "    for row in range(df.shape[0] - 1):\n",
    "        temp_row = df.ix[row,:]\n",
    "        for a in range(df.shape[1] - 2):\n",
    "            if temp_row.index[a] not in big_clus_freq.keys():\n",
    "                big_clus_freq[temp_row.index[a]] = temp_row[a]\n",
    "            else:\n",
    "                big_clus_freq[temp_row.index[a]] += temp_row[a]   \n",
    "    big_clus = pd.Series(big_clus_freq, index=big_clus_freq.keys())\n",
    "    big_clus.to_csv(\"big_clus\"+str(idx)+\".csv\")\n",
    "    print(\"big_clus\"+str(idx)+\".csv file saved!\")\n",
    "    idx += 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Separate_Clusters(big_clus1_df)\n",
    "Separate_Clusters(big_clus2_df)\n",
    "Separate_Clusters(big_clus3_df)\n",
    "Separate_Clusters(big_clus4_df)\n",
    "idx = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Big Cluster Analysis "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tm1 = [1 if x <= 9 else 0 for x in ddf.Membership]\n",
    "tm2 = [1 if x >= 10 and x <= 11 else 0 for x in ddf.Membership]\n",
    "tm3 = [1 if x >= 12 and x <= 15 else 0 for x in ddf.Membership]\n",
    "tm4 = [1 if x >= 16 else 0 for x in ddf.Membership]\n",
    "\n",
    "cat = 'fin'\n",
    "\n",
    "big_idx = 1\n",
    "def BigClusterAnal(tf, tm): # tf = his_tf, ddx_tf, etc\n",
    "    global big_idx\n",
    "    membership = fcluster(Z, 40, criterion='maxclust')\n",
    "    temp_tf = tf.T\n",
    "    temp_tf['Membership'] = membership.tolist() \n",
    "    \n",
    "    # Re-index Membership\n",
    "    temp_tf.Membership = tm\n",
    "    \n",
    "    # Build a decision tree\n",
    "    treeclf = tree.DecisionTreeClassifier(criterion='entropy', min_samples_split=25, min_impurity_split=0.00000002)\n",
    "    treeclf = treeclf.fit(temp_tf.ix[:, temp_tf.columns != 'Membership'], temp_tf['Membership'])\n",
    "\n",
    "    with open(\"haha_\"+str(big_idx)+\".dot\", 'w') as f:\n",
    "        f = tree.export_graphviz(treeclf, out_file=f)\n",
    "\n",
    "    os.unlink(\"haha_\"+str(big_idx)+\".dot\")\n",
    "\n",
    "    dot_data = tree.export_graphviz(treeclf, out_file=None) \n",
    "    graph = pydotplus.graph_from_dot_data(dot_data) \n",
    "    graph.write_pdf(\"haha_\"+str(big_idx)+\".pdf\")   \n",
    "    \n",
    "    term_imp = dict(zip(temp_tf.columns, treeclf.feature_importances_))\n",
    "    term_imp = sorted(term_imp.items(), key = lambda x: x[1], reverse = True) \n",
    "    \n",
    "    term_imp_df = pd.DataFrame.from_dict(term_imp, orient = \"columns\")\n",
    "    term_imp_df.columns = ['Term', 'Attr Score']\n",
    "    \n",
    "    term_imp_df['Attr Score'] = round((100 * (term_imp_df['Attr Score'] / np.max(term_imp_df['Attr Score']))))\n",
    "    \n",
    "    term_imp_df.to_csv(cat+\"_\"+str(big_idx)+\"_vs_all.csv\")\n",
    "    \n",
    "    print(cat+\"_\"+str(big_idx)+\"_vs_all.csv file saved!!\")\n",
    "    big_idx += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "BigClusterAnal(find_tf, tm1)\n",
    "BigClusterAnal(find_tf, tm2)\n",
    "BigClusterAnal(find_tf, tm3)\n",
    "BigClusterAnal(find_tf, tm4)\n",
    "\n",
    "big_idx = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Diagnosis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "membership = fcluster(Z, 40, criterion='maxclust')\n",
    "temp_tf = dia_tf.T\n",
    "temp_tf['Membership'] = membership.tolist() \n",
    "\n",
    "# Remove first two irrelevant columns\n",
    "temp_tf = temp_tf.ix[:,2:]\n",
    "\n",
    "ddf = temp_tf\n",
    "three_most_freq_terms = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ddf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Find Clusteroids\n",
    "# Iterate through all clusters\n",
    "\n",
    "clusteroids_list = []\n",
    "size_cluster = len(np.unique(ddf['Membership']))\n",
    "\n",
    "for c in range(1,41):\n",
    "    subset_cluster = ddf[ddf['Membership'] == c]\n",
    "    \n",
    "    spec_tf = [True if x in subset_cluster.index else False for x in dia_tf.columns]\n",
    "    term_lists = [dia_w[i] for i, x in enumerate(spec_tf) if x]\n",
    "    wc = {}\n",
    "    for term_list in term_lists:\n",
    "        wwc = term_list.split(' ')\n",
    "        for term in wwc:\n",
    "            if term == 'trauma' or term == 'tumor' or term == 'surgery' or term == 'inguinal_hernia' or term == 'bones':\n",
    "                continue\n",
    "            else:\n",
    "                if term.lower() not in wc.keys():\n",
    "                    wc[term.lower()] = 1\n",
    "                else:\n",
    "                    wc[term.lower()] = wc[term.lower()] + 1\n",
    "\n",
    "    count_asdict = sorted(wc.items(), key = lambda x: x[1], reverse = True) \n",
    "    #print the 3 most freq terms in DDX category\n",
    "#     print(str(c))\n",
    "\n",
    "    clean_count_asdict = count_asdict\n",
    "    print(str(c)+\"!++++++++++++++++++++++++++++\")\n",
    "    print(count_asdict)\n",
    "    print(\"++++++++++++++++++++++++++\")\n",
    "    three_most_freq_terms.append(count_asdict[:8])\n",
    "    \n",
    "#     del wc, count_asdict, term_lists # For memory efficiency\n",
    "\n",
    "# pd.DataFrame(dict(count_asdict[:5]).keys()).to_csv(\"tmft.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for c in range(1, 41):\n",
    "    subset_cluster = ddf[ddf['Membership'] == c]\n",
    "    km = KMeans(n_clusters=1, max_iter=1, random_state=33)\n",
    "    q = km.fit(subset_cluster.ix[:,subset_cluster.columns != \"Membership\"])\n",
    "    print(\"Cluster\"+str(c))\n",
    "    print(q.inertia_ / (subset_cluster.shape[0] - 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ddf[ddf.Membership == 4].columns[146]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Separate into 4 big clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Big clusters\n",
    "big_clus1_df = ddf[ddf.Membership <= 13]\n",
    "big_clus2_df = ddf[ddf.Membership >= 14]\n",
    "big_clus2_df = big_clus2_df[big_clus2_df.Membership <= 16]\n",
    "big_clus3_df = ddf[ddf.Membership >= 17]\n",
    "big_clus3_df = big_clus3_df[big_clus3_df.Membership <= 34]\n",
    "big_clus4_df = ddf[ddf.Membership >= 35]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "idx = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def Separate_Clusters(df):\n",
    "    global idx\n",
    "    big_clus_freq = {}\n",
    "    for row in range(df.shape[0] - 1):\n",
    "        temp_row = df.ix[row,:]\n",
    "        for a in range(df.shape[1] - 2):\n",
    "            if temp_row.index[a] not in big_clus_freq.keys():\n",
    "                big_clus_freq[temp_row.index[a]] = temp_row[a]\n",
    "            else:\n",
    "                big_clus_freq[temp_row.index[a]] += temp_row[a]   \n",
    "    big_clus = pd.Series(big_clus_freq, index=big_clus_freq.keys())\n",
    "    big_clus.to_csv(\"big_clus\"+str(idx)+\".csv\")\n",
    "    print(\"big_clus\"+str(idx)+\".csv file saved!\")\n",
    "    idx += 1    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Separate_Clusters(big_clus1_df)\n",
    "Separate_Clusters(big_clus2_df)\n",
    "Separate_Clusters(big_clus3_df)\n",
    "Separate_Clusters(big_clus4_df)\n",
    "\n",
    "idx = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Big Cluster Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tm1 = [1 if x <= 13 else 0 for x in ddf.Membership]\n",
    "tm2 = [1 if x >= 14 and x <= 16 else 0 for x in ddf.Membership]\n",
    "tm3 = [1 if x >= 17 and x <= 34 else 0 for x in ddf.Membership]\n",
    "tm4 = [1 if x >= 35 else 0 for x in ddf.Membership]\n",
    "\n",
    "cat = 'dia'\n",
    "\n",
    "big_idx = 1\n",
    "def BigClusterAnal(tf, tm): # tf = his_tf, ddx_tf, etc\n",
    "    global big_idx\n",
    "    membership = fcluster(Z, 50, criterion='maxclust')\n",
    "    temp_tf = tf.T\n",
    "    temp_tf['Membership'] = membership.tolist() \n",
    "    \n",
    "    # Re-index Membership\n",
    "    temp_tf.Membership = tm\n",
    "    \n",
    "    # Build a decision tree\n",
    "    treeclf = tree.DecisionTreeClassifier(criterion='entropy', min_samples_split=25, min_impurity_split=0.00000002)\n",
    "    treeclf = treeclf.fit(temp_tf.ix[:, temp_tf.columns != 'Membership'], temp_tf['Membership'])\n",
    "\n",
    "    with open(\"haha_\"+str(big_idx)+\".dot\", 'w') as f:\n",
    "        f = tree.export_graphviz(treeclf, out_file=f)\n",
    "\n",
    "    os.unlink(\"haha_\"+str(big_idx)+\".dot\")\n",
    "\n",
    "    dot_data = tree.export_graphviz(treeclf, out_file=None) \n",
    "    graph = pydotplus.graph_from_dot_data(dot_data) \n",
    "    graph.write_pdf(\"haha_\"+str(big_idx)+\".pdf\")   \n",
    "    \n",
    "    term_imp = dict(zip(temp_tf.columns, treeclf.feature_importances_))\n",
    "    term_imp = sorted(term_imp.items(), key = lambda x: x[1], reverse = True) \n",
    "    \n",
    "    term_imp_df = pd.DataFrame.from_dict(term_imp, orient = \"columns\")\n",
    "    term_imp_df.columns = ['Term', 'Attr Score']\n",
    "    \n",
    "    term_imp_df['Attr Score'] = round((100 * (term_imp_df['Attr Score'] / np.max(term_imp_df['Attr Score']))))\n",
    "    \n",
    "    term_imp_df.to_csv(cat+\"_\"+str(big_idx)+\"_vs_all.csv\")\n",
    "    \n",
    "    print(cat+\"_\"+str(big_idx)+\"_vs_all.csv file saved!!\")\n",
    "    big_idx += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "BigClusterAnal(dia_tf, tm1)\n",
    "BigClusterAnal(dia_tf, tm2)\n",
    "BigClusterAnal(dia_tf, tm3)\n",
    "BigClusterAnal(dia_tf, tm4)\n",
    "\n",
    "big_idx = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discussion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "membership = fcluster(Z, 60, criterion='maxclust')\n",
    "temp_tf = dis_tf.T\n",
    "temp_tf['Membership'] = membership.tolist() \n",
    "\n",
    "# Remove first two irrelevant columns\n",
    "temp_tf = temp_tf.ix[:,2:]\n",
    "\n",
    "ddf = temp_tf\n",
    "three_most_freq_terms = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Find Clusteroids\n",
    "# Iterate through all clusters\n",
    "\n",
    "clusteroids_list = []\n",
    "size_cluster = len(np.unique(ddf['Membership']))\n",
    "\n",
    "for c in range(1,61):\n",
    "    subset_cluster = ddf[ddf['Membership'] == c]\n",
    "    \n",
    "    spec_tf = [True if x in subset_cluster.index else False for x in dis_tf.columns]\n",
    "    term_lists = [dis_w[i] for i, x in enumerate(spec_tf) if x]\n",
    "    wc = {}\n",
    "    for term_list in term_lists:\n",
    "        wwc = term_list.split(' ')\n",
    "        for term in wwc:\n",
    "            if term == 'trauma' or term == 'tumor' or term == 'surgery' or term == 'inguinal_hernia' or term == 'bones':\n",
    "                continue\n",
    "            else:\n",
    "                if term.lower() not in wc.keys():\n",
    "                    wc[term.lower()] = 1\n",
    "                else:\n",
    "                    wc[term.lower()] = wc[term.lower()] + 1\n",
    "\n",
    "    count_asdict = sorted(wc.items(), key = lambda x: x[1], reverse = True) \n",
    "    #print the 3 most freq terms in DDX category\n",
    "#     print(str(c))\n",
    "\n",
    "    clean_count_asdict = count_asdict\n",
    "    print(str(c)+\"!++++++++++++++++++++++++++++\")\n",
    "    print(count_asdict)\n",
    "    print(\"++++++++++++++++++++++++++\")\n",
    "    three_most_freq_terms.append(count_asdict[:8])\n",
    "    \n",
    "#     del wc, count_asdict, term_lists # For memory efficiency\n",
    "\n",
    "# pd.DataFrame(dict(count_asdict[:5]).keys()).to_csv(\"tmft.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for c in range(1, 61):\n",
    "    subset_cluster = ddf[ddf['Membership'] == c]\n",
    "    km = KMeans(n_clusters=1, max_iter=1, random_state=33)\n",
    "    q = km.fit(subset_cluster.ix[:,subset_cluster.columns != \"Membership\"])\n",
    "    print(\"Cluster\"+str(c))\n",
    "    print(q.inertia_ / (subset_cluster.shape[0] - 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Separate into 11 big clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Big clusters\n",
    "big_clus1_df = ddf[ddf.Membership <= 1]\n",
    "big_clus2_df = ddf[ddf.Membership >= 2]\n",
    "big_clus2_df = big_clus2_df[big_clus2_df.Membership <= 4]\n",
    "big_clus3_df = ddf[ddf.Membership >= 5]\n",
    "big_clus3_df = big_clus3_df[big_clus3_df.Membership <= 9]\n",
    "big_clus4_df = ddf[ddf.Membership >= 10]\n",
    "big_clus4_df = big_clus4_df[big_clus4_df.Membership <= 19]\n",
    "big_clus5_df = ddf[ddf.Membership >= 20]\n",
    "big_clus5_df = big_clus5_df[big_clus5_df.Membership <= 34]\n",
    "big_clus6_df = ddf[ddf.Membership >= 35]\n",
    "big_clus6_df = big_clus6_df[big_clus6_df.Membership <= 41]\n",
    "big_clus7_df = ddf[ddf.Membership >= 42]\n",
    "big_clus7_df = big_clus7_df[big_clus7_df.Membership <= 43]\n",
    "big_clus8_df = ddf[ddf.Membership >= 44]\n",
    "big_clus8_df = big_clus8_df[big_clus8_df.Membership <= 53]\n",
    "big_clus9_df = ddf[ddf.Membership >= 54]\n",
    "big_clus9_df = big_clus9_df[big_clus9_df.Membership <= 55]\n",
    "big_clus10_df = ddf[ddf.Membership >= 56]\n",
    "big_clus10_df = big_clus10_df[big_clus10_df.Membership <= 57]\n",
    "big_clus11_df = ddf[ddf.Membership >= 58]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "idx = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def Separate_Clusters(df):\n",
    "    global idx\n",
    "    big_clus_freq = {}\n",
    "    for row in range(df.shape[0] - 1):\n",
    "        temp_row = df.ix[row,:]\n",
    "        for a in range(df.shape[1] - 2):\n",
    "            if temp_row.index[a] not in big_clus_freq.keys():\n",
    "                big_clus_freq[temp_row.index[a]] = temp_row[a]\n",
    "            else:\n",
    "                big_clus_freq[temp_row.index[a]] += temp_row[a]   \n",
    "    big_clus = pd.Series(big_clus_freq, index=big_clus_freq.keys())\n",
    "    big_clus.to_csv(\"big_clus\"+str(idx)+\".csv\")\n",
    "    print(\"big_clus\"+str(idx)+\".csv file saved!\")\n",
    "    idx += 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Separate_Clusters(big_clus1_df)\n",
    "Separate_Clusters(big_clus2_df)\n",
    "Separate_Clusters(big_clus3_df)\n",
    "Separate_Clusters(big_clus4_df)\n",
    "Separate_Clusters(big_clus5_df)\n",
    "Separate_Clusters(big_clus6_df)\n",
    "Separate_Clusters(big_clus7_df)\n",
    "Separate_Clusters(big_clus8_df)\n",
    "Separate_Clusters(big_clus9_df)\n",
    "Separate_Clusters(big_clus10_df)\n",
    "Separate_Clusters(big_clus11_df)\n",
    "\n",
    "idx = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Big Cluster Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tm1 = [1 if x <= 1 else 0 for x in ddf.Membership]\n",
    "tm2 = [1 if x >= 2 and x <= 4 else 0 for x in ddf.Membership]\n",
    "tm3 = [1 if x >= 5 and x <= 9 else 0 for x in ddf.Membership]\n",
    "tm4 = [1 if x >= 10 and x <= 19 else 0 for x in ddf.Membership]\n",
    "tm5 = [1 if x >= 20 and x <= 34 else 0 for x in ddf.Membership]\n",
    "tm6 = [1 if x >= 35 and x <= 41 else 0 for x in ddf.Membership]\n",
    "tm7 = [1 if x >= 42 and x <= 43 else 0 for x in ddf.Membership]\n",
    "tm8 = [1 if x >= 44 and x <= 53 else 0 for x in ddf.Membership]\n",
    "tm9 = [1 if x >= 54 and x <= 55 else 0 for x in ddf.Membership]\n",
    "tm10 = [1 if x >= 56 and x <= 57 else 0 for x in ddf.Membership]\n",
    "tm11 = [1 if x >= 58 else 0 for x in ddf.Membership]\n",
    "\n",
    "cat = 'dis'\n",
    "\n",
    "big_idx = 1\n",
    "def BigClusterAnal(tf, tm): # tf = his_tf, ddx_tf, etc\n",
    "    global big_idx\n",
    "    membership = fcluster(Z, 50, criterion='maxclust')\n",
    "    temp_tf = tf.T\n",
    "    temp_tf['Membership'] = membership.tolist() \n",
    "    \n",
    "    # Re-index Membership\n",
    "    temp_tf.Membership = tm\n",
    "    \n",
    "    # Build a decision tree\n",
    "    treeclf = tree.DecisionTreeClassifier(criterion='entropy', min_samples_split=25, min_impurity_split=0.00000002)\n",
    "    treeclf = treeclf.fit(temp_tf.ix[:, temp_tf.columns != 'Membership'], temp_tf['Membership'])\n",
    "\n",
    "    with open(\"haha_\"+str(big_idx)+\".dot\", 'w') as f:\n",
    "        f = tree.export_graphviz(treeclf, out_file=f)\n",
    "\n",
    "    os.unlink(\"haha_\"+str(big_idx)+\".dot\")\n",
    "\n",
    "    dot_data = tree.export_graphviz(treeclf, out_file=None) \n",
    "    graph = pydotplus.graph_from_dot_data(dot_data) \n",
    "    graph.write_pdf(\"haha_\"+str(big_idx)+\".pdf\")   \n",
    "    \n",
    "    term_imp = dict(zip(temp_tf.columns, treeclf.feature_importances_))\n",
    "    term_imp = sorted(term_imp.items(), key = lambda x: x[1], reverse = True) \n",
    "    \n",
    "    term_imp_df = pd.DataFrame.from_dict(term_imp, orient = \"columns\")\n",
    "    term_imp_df.columns = ['Term', 'Attr Score']\n",
    "    \n",
    "    term_imp_df['Attr Score'] = round((100 * (term_imp_df['Attr Score'] / np.max(term_imp_df['Attr Score']))))\n",
    "    \n",
    "    term_imp_df.to_csv(cat+\"_\"+str(big_idx)+\"_vs_all.csv\")\n",
    "    \n",
    "    print(cat+\"_\"+str(big_idx)+\"_vs_all.csv file saved!!\")\n",
    "    big_idx += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "BigClusterAnal(dis_tf, tm1)\n",
    "BigClusterAnal(dis_tf, tm2)\n",
    "BigClusterAnal(dis_tf, tm3)\n",
    "BigClusterAnal(dis_tf, tm4)\n",
    "BigClusterAnal(dis_tf, tm5)\n",
    "BigClusterAnal(dis_tf, tm6)\n",
    "BigClusterAnal(dis_tf, tm7)\n",
    "BigClusterAnal(dis_tf, tm8)\n",
    "BigClusterAnal(dis_tf, tm9)\n",
    "BigClusterAnal(dis_tf, tm10)\n",
    "BigClusterAnal(dis_tf, tm11)\n",
    "\n",
    "big_idx = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ddf.columns[22]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cluster Evaluation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DDX "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "within = []\n",
    "between = []\n",
    "for k in range(10, 310, 10):\n",
    "    ddx_kmeans_cluster = Clustering(ddx_tf, ddx_tfidf, k, False)\n",
    "    within.append(ddx_kmeans_cluster.inertia_)\n",
    "    bd = Clust_Eval(ddx_kmeans_cluster, k)\n",
    "    between.append(bd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "btwn_within_ratio = np.divide(between, within)\n",
    "plt.plot([x for x in range(10, 310, 10)], btwn_within_ratio)\n",
    "plt.title('Between-Within Ratio')\n",
    "plt.xlabel('Number of clusters')\n",
    "plt.ylabel('Ratio')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RadLex Category Tracker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "example = []\n",
    "for terms in three_most_freq_terms:\n",
    "    for term in terms:\n",
    "        if term[0] == \"\":\n",
    "            example.append(\"abscess\")\n",
    "        elif term[0] == \"polyp\":\n",
    "            example.append(\"lateral\")\n",
    "        else:\n",
    "            example.append(term[0])\n",
    "\n",
    "# Remove all underline\n",
    "example = [tm.replace(\"_\", \" \") for tm in example]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Modify some terms in a list\n",
    "flag = 0\n",
    "for x, tt in enumerate(example):\n",
    "    if flag == 0 and tt == \"fossa\":\n",
    "        example[x] = \"fourth ventricle\"\n",
    "        flag = 1\n",
    "        print(\"CHANGE\")\n",
    "    elif flag == 1 and tt == \"fossa\":\n",
    "        example[x] = \"fourth ventricle\"\n",
    "        flag = 2\n",
    "        print(\"CHANGE one more\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Re-define variable\n",
    "radlex_term_category_list = pd.read_csv(\"Radlex.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "term_category_dict = {}\n",
    "clus_idx = 1\n",
    "for x, term in enumerate(example):\n",
    "    hier = \"\"\n",
    "    terms = \"\"\n",
    "\n",
    "    target = radlex_term_category_list[radlex_term_category_list[\"Name or Synonym\"].values == term]\n",
    "    hier += target[\"Name or Synonym\"].values+\" => \"\n",
    "    terms += target[\"Name or Synonym\"].values+\",\"\n",
    "    # Loop through until it reaches to the root\n",
    "\n",
    "    try:\n",
    "        while target[\"Parent RID\"].values[0] != \"RID1\":\n",
    "            parent_rid = target[\"Parent RID\"].values[0]\n",
    "            target = radlex_term_category_list[radlex_term_category_list[\"RID\"] == parent_rid]\n",
    "#             print(target)\n",
    "            hier += target[\"Name or Synonym\"].values+\" => \"\n",
    "            terms += target[\"Name or Synonym\"].values+\",\"\n",
    "\n",
    "        terms_list = unlist(terms).strip().split(\",\")\n",
    "\n",
    "        terms_list = terms_list[1:-1]\n",
    "\n",
    "        if terms_list[-1] not in term_category_dict.keys():\n",
    "            term_category_dict[terms_list[-1]] = {}\n",
    "            for term in terms_list:\n",
    "                if term not in term_category_dict[terms_list[-1]].keys():\n",
    "                    term_category_dict[terms_list[-1]][term] = 1\n",
    "                else:\n",
    "                    term_category_dict[terms_list[-1]][term] += 1\n",
    "        else:\n",
    "            for term in terms_list:\n",
    "                if term not in term_category_dict[terms_list[-1]].keys():\n",
    "                    term_category_dict[terms_list[-1]][term] = 1\n",
    "                else:\n",
    "                    term_category_dict[terms_list[-1]][term] += 1\n",
    "\n",
    "        if x % 8 == 0:\n",
    "            print(\"============CLUSTER \"+str(clus_idx)+\"==============\")\n",
    "            print(\"===============\")\n",
    "            clus_idx += 1\n",
    "\n",
    "        print(hier)\n",
    "    except IndexError:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Find Clusteroids\n",
    "# Iterate through all clusters\n",
    "\n",
    "clusteroids_list = []\n",
    "size_cluster = len(np.unique(ddf['Membership']))\n",
    "\n",
    "for c in range(1,61):\n",
    "    subset_cluster = ddf[ddf['Membership'] == c]\n",
    "    \n",
    "    spec_tf = [True if x in subset_cluster.index else False for x in dis_tf.columns]\n",
    "    term_lists = [dis_w[i] for i, x in enumerate(spec_tf) if x]\n",
    "    wc = {}\n",
    "    for term_list in term_lists:\n",
    "        wwc = term_list.split(' ')\n",
    "        for term in wwc:\n",
    "            if term == 'trauma' or term == 'tumor' or term == 'surgery' or term == 'inguinal_hernia' or term == 'bones':\n",
    "                continue\n",
    "            else:\n",
    "                if term.lower() not in wc.keys():\n",
    "                    wc[term.lower()] = 1\n",
    "                else:\n",
    "                    wc[term.lower()] = wc[term.lower()] + 1\n",
    "\n",
    "    count_asdict = sorted(wc.items(), key = lambda x: x[1], reverse = True) \n",
    "    #print the 3 most freq terms in DDX category\n",
    "#     print(str(c))\n",
    "\n",
    "    clean_count_asdict = count_asdict\n",
    "    print(str(c)+\"!++++++++++++++++++++++++++++\")\n",
    "    print(count_asdict)\n",
    "    print(\"++++++++++++++++++++++++++\")\n",
    "    three_most_freq_terms.append(count_asdict[:8])\n",
    "    \n",
    "#     del wc, count_asdict, term_lists # For memory efficiency\n",
    "\n",
    "# pd.DataFrame(dict(count_asdict[:5]).keys()).to_csv(\"tmft.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  },
  "nav_menu": {},
  "toc": {
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 6,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": true
  },
  "toc_position": {
   "height": "656px",
   "left": "0px",
   "right": "auto",
   "top": "106px",
   "width": "212px"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
